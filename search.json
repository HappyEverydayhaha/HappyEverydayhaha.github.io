[{"title":"Hello World","url":"/2025/08/27/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"智学DS-AI文档","url":"/2025/08/27/%E6%99%BA%E5%AD%A6DS-AI%E6%96%87%E6%A1%A3/","content":"一、两大核心微调模型详解我们在 deepseek-llm:7b-chat 这个强大的开源基础模型之上，通过先进的 QLoRA (4位量化低秩适配) 技术，精心微调出了两个针对“智学DS”项目特定需求的专家模型。这种参数高效微调技术允许我们在消费级硬件上，以极高的效率定制大模型，同时不损伤其原有的通用能力。\n1. DeepSeek-LoRA-Chat (概念讲解与风格化模型)\n微调过程：该模型使用了我们手工构建（辅以SOTA模型Gemini 2.5 pro蒸馏）的SFT（监督微调）指令数据集。这个数据集的核心是**“一问一答”式的教学内容，我们精心编写了280条关于数据结构核心概念的问答对。特别地，在回答中我们融入了大量生动有趣的比喻**，旨在让枯燥的概念变得形象易懂。训练过程采用 LoRA 技术，在基础模型上“嫁接”了一个学习这种教学风格的适配器。\n其中数据集的一条问答对的样例如下所示。\n&#123;&quot;instruction&quot;: &quot;在C语言中，指针到底是什么？请用一个简单的比喻解释。&quot;, &quot;output&quot;: &quot;指针就像一张写着地址的“便签”。想象一下，你朋友的家是一个变量，它储存在某个地方。你朋友家的地址就是指针的值。通过这张“便签”（指针），你可以直接找到他的家（访问变量），而不需要知道他家的具体门牌号。简单来说，指针本身不存储数据，它存储的是存储数据的那个“地方”的地址。&quot;&#125;\n\nLoRA 配置细节：\n\n秩 (Rank r): 设置为 16。这是一个相对较小的秩，旨在用较少的参数（约0.1-0.2%的可训练参数）捕捉特定的语言风格和基础概念，防止模型在通用能力上发生“灾难性遗忘”。\nAlpha (α): 设置为 32。alpha 是LoRA的缩放因子，通常设为 r 的两倍，用于增强微调的效果。\n目标模块 (Target Modules)：我们精确地将LoRA适配器应用于模型中所有负责特征提取和注意力计算的线性层，包括 q_proj, k_proj, v_proj, o_proj，以最高效地影响模型的生成行为。\n\n\n训练过程：\n\n训练框架：使用 Hugging Face Trainer。\n批处理大小 (Batch Size)：有效批大小为 16 (通过 per_device_train_batch_size&#x3D;2 和 gradient_accumulation_steps&#x3D;8 实现)，在保证梯度稳定性的同时优化了显存占用。\n训练步数：总共训练了 200步 (steps)，确保模型对这批核心数据有充分的学习。\n\n\n特色亮点：\n\n风格化教学：模型的核心特色是其风趣、形象的语言风格。它不再是冷冰冰的知识复读机，而更像一位擅长打比方的“网红教师”，能有效降低学生对复杂概念的理解门槛。\n概念大师：专注于数据结构核心概念的清晰阐释，回答内容聚焦于“是什么”和“为什么”，而非复杂的代码实现。\n高效率：由于数据集相对简短，微调过程快速，迭代优化方便。\n\n\n对“智学DS”项目的作用意义：这个模型在项目中可以扮演着**“学习兴趣激发者”和“概念入门导师”的关键角色。在学生学习的初期阶段，使用该模型进行AI互动问答，能够有效破除畏难情绪，建立学习兴趣**。它是将“智学DS”从一个纯粹的工具平台，提升为一个具有亲和力和教学温度的智能学习伴侣的核心组件。\n\n有趣的结果：\n\n\n2. DeepSeek-LoRA-CoT-Code (代码生成与逻辑分析模型)\n微调过程：该模型基于我们构建的更高级的CoT（思维链）SFT指令数据集进行训练。这个数据集的每一条样本，都严格遵循 “【思考】→【规划】→【代码实现】→【解释】” 的四段式结构。我们投入大量精力编写了100条数据，模拟了一位资深程序员和教师在解决编程问题时的完整思考链路，特别聚焦于C语言数据结构的代码生成与算法逻辑分析。\n其中数据集的一条数据如下所示。\n&#123;&quot;instruction&quot;: &quot;请用C语言实现冒泡排序算法。&quot;, &quot;output&quot;: &quot;【思考】\\n冒泡排序是一种简单的排序算法。它的核心思想是重复地遍历待排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。\\n\\n【规划】\\n1.  编写一个 `bubbleSort` 函数，接收一个整型数组和它的长度。\\n2.  使用两层嵌套循环。外层循环控制总共需要进行多少“趟”排序。对于n个元素，最多需要 n-1 趟。\\n3.  内层循环负责在每一趟中进行相邻元素的比较和交换。内层循环的边界可以优化：因为每一趟都会将一个最大的元素“沉”到末尾，所以下一趟的比较就不需要再包含已经排好序的末尾部分。\\n4.  为了进一步优化，可以设置一个标志位 `swapped`。如果在某一趟排序中，一次交换都没有发生，说明整个数列已经有序，可以直接提前结束外层循环。\\n\\n【代码实现】\\n```c\\n#include &lt;stdio.h&gt;\\n\\n// 交换两个整数的辅助函数\\nvoid swap(int *xp, int *yp) &#123;\\n    int temp = *xp;\\n    *xp = *yp;\\n    *yp = temp;\\n&#125;\\n\\n// 冒泡排序函数\\nvoid bubbleSort(int arr[], int n) &#123;\\n    int i, j;\\n    int swapped;\\n    for (i = 0; i &lt; n - 1; i++) &#123;\\n        swapped = 0; // 每一趟开始前，假定没有发生交换\\n        // 内层循环：从头开始比较相邻元素，把最大的元素放到末尾\\n        // -i-1 是因为每趟排序后，末尾的i个元素已经就位\\n        for (j = 0; j &lt; n - i - 1; j++) &#123;\\n            if (arr[j] &gt; arr[j + 1]) &#123;\\n                swap(&amp;arr[j], &amp;arr[j + 1]);\\n                swapped = 1; // 发生了交换\\n            &#125;\\n        &#125;\\n\\n        // 如果在一趟中没有发生任何交换，则数组已经有序\\n        if (swapped == 0) &#123;\\n            break;\\n        &#125;\\n    &#125;\\n&#125;\\n\\n// 打印数组的辅助函数\\nvoid printArray(int arr[], int size) &#123;\\n    int i;\\n    for (i = 0; i &lt; size; i++) &#123;\\n        printf(\\&quot;%d \\&quot;, arr[i]);\\n    &#125;\\n    printf(\\&quot;\\\\n\\&quot;);\\n&#125;\\n\\nint main() &#123;\\n    int arr[] = &#123;64, 34, 25, 12, 22, 11, 90&#125;;\\n    int n = sizeof(arr) / sizeof(arr);\\n    bubbleSort(arr, n);\\n    printf(\\&quot;排序后的数组: \\\\n\\&quot;);\\n    printArray(arr, n);\\n    return 0;\\n&#125;\\n```\\n\\n【解释】\\n冒泡排序通过嵌套循环实现。外层循环 `i` 控制排序的趟数，总共需要 `n-1` 趟。内层循环 `j` 负责每一趟的比较和交换。它从数组的第一个元素开始，比较 `arr[j]` 和 `arr[j+1]`，如果前者大于后者，则交换它们。经过第一趟后，最大的元素会被移动到数组的末尾。第二趟则会将第二大的元素移动到倒数第二个位置，以此类推。代码中的 `swapped` 标志是一个优化，用于检测数组是否已提前有序，从而避免不必要的循环。冒泡排序的时间复杂度在最坏和平均情况下都是 O(n²)，最好情况下为 O(n)（如果数组已经有序且加入了标志位优化）。由于其简单性，它常用于教学，但在实际应用中对于大数据集效率较低。&quot;&#125;\n\nLoRA 配置细节：\n\n与 LoRA-Chat 模型保持一致的配置（r&#x3D;16, alpha&#x3D;32），以确保比较的公平性，并验证CoT数据本身带来的性能提升。\n目标模块同样锁定了所有关键的注意力层和前馈网络层。\n\n\n训练过程：\n\n训练框架：同样使用 Hugging Face Trainer。\n序列长度 (Max Seq Length)：由于CoT数据包含大量文本，我们将最大序列长度设置为 1024 tokens，以容纳完整的思考链路。\n训练轮次 (Epochs)：鉴于CoT数据的信息密度和长度，我们没有采用固定步数，而是训练了 3个完整的轮次 (epochs)。而3个epochs通常被认为是高质量小数据集微调的“甜点区”（sweet spot），这能保证模型对每一条高质量的CoT样本都有充分的“消化吸收”，使其能够学习样本内在的逻辑推理模式；又不至于过拟合，丧失创造性和泛化能力。\n保存策略：采用 save_strategy&#x3D;”steps”，save_steps&#x3D;2，便于断点续算，也便于观察模型在不同训练阶段的性能演进。\n\n\n特色亮点：\n\n逻辑推理透明化：最大的亮点在于，模型不仅能给出最终代码，更能一定程度上展示其“思考过程”。这使得代码结果不再是一个“黑盒子”，学生可以清晰地看到从问题分析到分步规划，再到最终实现的全过程。\n代码能力强化：通过对大量高质量、带注释的C语言代码进行微调，模型生成的代码在规范性、准确性和可读性上比原始模型更优，更适合教学场景。\n严谨专业：模型的整体风格被校准为严谨、准确、专业，符合编程教学的客观要求。\n\n\n对“智学DS”项目的作用意义：这个模型是**“编程实践指导者”和“算法思维训练师”。它将AI的能力从“给出答案”提升到了“教授方法”的层面。在学生进行编程练习和算法学习时，该模型能提供可复现的、结构化的解题思路**，这对于培养学生的计算思维和独立解决问题的能力具有重要的价值。\n\n有趣的结果：\n\n\n\n二、项目所涉语言模型（LLM）全景解析“智学DS”平台采用了一种混合AI模型的策略，结合了本地模型的私密性、云端模型的强大能力以及RAG技术的可控性，形成了一个优势互补、功能全面的AI矩阵。\n\ndeepseek-7b-chat-original (原始基座模型)\n定位：性能基准与对照组。\n描述：这是未经任何修改的官方 deepseek-llm:7b-chat 模型，经过4-bit量化以在本地高效运行。它是我们所有本地微调工作的起点。在“大模型竞技场”中，它的存在让我们能直观地衡量出我们的微调工作（LoRA-SFT 和 LoRA-CoT）到底带来了多大的性能提升和风格变化。\n\n\ndeepseek:7b-chat-lora (SFT微调模型)\n定位：概念讲解专家。\n描述：这个模型通过SFT微调，专注于用生动形象的语言解释数据结构概念。它牺牲了一部分通用性，换取了在特定教学风格上的专精。\n\n\ndeepseek:7b-cot-code-lora (CoT微调模型)\n定位：代码与逻辑分析专家。\n描述：这个模型通过思维链微调，强化了其代码生成和逻辑推理能力，回答风格严谨、步骤清晰，是项目中的一大技术亮点。\n\n\ndeepseek-rag (本地RAG模型)\n定位：本地知识库问答系统。\n描述：它将原始的 deepseek-llm:7b-chat 模型与一个本地的、由 bge-m3 向量化的内置知识库相结合。它不依赖外部网络，所有回答都基于我们预置的、可信的教学材料。这保证了知识的准确性和数据隐私，非常适合处理那些需要严格遵循教学大纲的核心问题。\n\n\nqwen-turbo (云端知识库RAG模型)\n定位：通用问答与联网搜索主力。\n描述：这是一个在阿里云百炼平台上的强大应用，连接了我们上传在云端的大规模知识库（包含更多课程相关PPT，课程资料pdf等内容），可以利用行业先进的多格式文件解析系统实现更好的RAG效果。它拥有更广阔的知识面和更强的通用对话能力，并且我们还为其开启了联网搜索功能，使其能够回答关于最新技术或外部链接的知识性问题，弥补了知识库更新不及时的短板。\n\n\nqwen-long-rag (云端长文本模型 + 本地RAG)\n定位：核心AI功能驱动引擎。\n描述：这是项目后端功能（如AI生成教学内容、AI生成习题等）的核心驱动。我们选择 Qwen-Long 模型是因为它拥有超长的上下文窗口与较强的指令遵循能力，不仅能够一次性处理和理解教师上传的整篇文档或完整的教学内容，也能够精确地生成指定格式的习题集等。并且它同样结合了我们本地的RAG知识库，确保了在进行内容创作和习题生成时，能够优先、准确地使用我们提供的核心教学材料。\n\n\nqwen-vl-max (云端多模态大模型)\n定位：AI智能评阅与图像理解核心引擎。\n描述：这是项目负责处理图文混合内容的模型。我们选择 qwen-vl-max 是因为它是一款强大的多模态大模型，具备较强的视觉语言（Vision-Language）与协同理解能力。在“智学DS”中，它的核心任务是AI自动评阅，特别是当学生上传包含手写公式、数据结构示意图等图片作为答案时，qwen-vl-max 能够同时分析题干、标准答案、学生的文本回答以及图片内容，给出精准的分数和评价。\n\n\n\n\n三、核心AI架构选择的考量为什么我们选择 “云端Qwen-Long + 本地bge-m3的RAG架构” 作为核心AI驱动？\n这是一个经过深思熟虑的、兼顾了性能、成本、准确性和可扩展性的架构决策。\n\n为什么选择云端 Qwen-Long 作为生成模型 (LLM)？\n\n长上下文处理能力 (核心)：教学场景的核心任务，如“根据这份30页的PDF课件生成一份教学大纲和配套习题”，需要模型能够一次性“读懂”非常长的文本。Qwen-Long 专为此类长文本任务设计，其超长的上下文窗口是常规模型难以企及的。这保证了AI生成内容的连贯性和完整性。\n强大的基础能力：作为顶级的中文商业化云端模型，Qwen-Long 的参数量被推测处于数千亿（Hundreds of Billions）到万亿（Trillion）级别，并且大概率是一个复杂的 MoE (Mixture of Experts) 架构，其在语言理解、逻辑推理和内容创作方面的基础能力几乎超过现有各种的开源模型。这直接决定了AI生成的教学内容、习题和评语的质量上限。\n避免本地资源瓶颈：复杂的生成任务对本地GPU资源消耗巨大且耗时很长。将其放在云端，可以为用户提供快速、稳定的服务体验，而无需担心用户本地硬件的限制。\n\n\n为什么选择本地 bge-m3 进行向量化？\n\n卓越的性能：bge-m3 (BAAI General Embedding) 是目前业界公认的、性能顶尖的开源文本向量化模型。它在中英文混合场景下的语义理解和检索能力非常强大，能确保在RAG流程的第一步——“检索”——就能精准地找到与用户问题最相关的知识片段。\n本地化与数据隐私：知识库（教学材料）是学校或教师的核心资产。通过在服务器本地使用 bge-m3 进行向量化，并将向量索引存储在本地的 ChromaDB 中，我们确保了教师的私有教学材料数据永远不会离开我们的服务器，不会被上传到任何第三方云服务商进行处理。这最大限度地保障了数据安全和隐私。\n成本效益与灵活性：bge-m3 是开源免费的，在本地部署没有API调用费用。同时，这也给了我们未来替换或升级向量化模型的完全自由。\n\n\n\n总结：我们的核心AI架构是一个典型的“混合云”+“本地化控制”的最佳实践。\n我们利用云端大模型 (Qwen-Long) 强大的“大脑”来执行复杂的生成任务，同时通过本地化、高性能的向量模型 (bge-m3) 和知识库来牢牢掌控知识的准确性、安全性和私密性。这种架构让“智学DS”平台在提供高质量AI功能的同时，也建立起了坚实的数据安全壁垒，是一个技术先进且商业上稳健的选择。\n","categories":["SoftCup"],"tags":["AI","SoftCup"]}]